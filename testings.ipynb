{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/siz075/.local/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/28 10:50:05 PM episode 0 score 42.0 avg score 42.0\n",
      "02/28 10:50:05 PM episode 1 score 16.0 avg score 29.0\n",
      "02/28 10:50:05 PM episode 2 score 17.0 avg score 25.0\n",
      "02/28 10:50:05 PM episode 3 score 20.0 avg score 23.8\n",
      "02/28 10:50:05 PM episode 4 score 36.0 avg score 26.2\n",
      "02/28 10:50:05 PM episode 5 score 17.0 avg score 24.7\n",
      "02/28 10:50:05 PM episode 6 score 20.0 avg score 24.0\n",
      "02/28 10:50:05 PM episode 7 score 28.0 avg score 24.5\n",
      "02/28 10:50:05 PM episode 8 score 27.0 avg score 24.8\n",
      "02/28 10:50:05 PM episode 9 score 23.0 avg score 24.6\n",
      "02/28 10:50:06 PM episode 10 score 22.0 avg score 24.4\n",
      "02/28 10:50:06 PM episode 11 score 15.0 avg score 23.6\n",
      "02/28 10:50:07 PM episode 12 score 27.0 avg score 23.8\n",
      "02/28 10:50:08 PM episode 13 score 17.0 avg score 23.4\n",
      "02/28 10:50:09 PM episode 14 score 18.0 avg score 23.0\n",
      "02/28 10:50:09 PM episode 15 score 17.0 avg score 22.6\n",
      "02/28 10:50:10 PM episode 16 score 17.0 avg score 22.3\n",
      "02/28 10:50:10 PM episode 17 score 9.0 avg score 21.6\n",
      "02/28 10:50:12 PM episode 18 score 41.0 avg score 22.6\n",
      "02/28 10:50:12 PM episode 19 score 20.0 avg score 22.4\n",
      "02/28 10:50:14 PM episode 20 score 50.0 avg score 23.8\n",
      "02/28 10:50:15 PM episode 21 score 22.0 avg score 23.7\n",
      "02/28 10:50:16 PM episode 22 score 44.0 avg score 24.6\n",
      "02/28 10:50:18 PM episode 23 score 73.0 avg score 26.6\n",
      "02/28 10:50:19 PM episode 24 score 26.0 avg score 26.6\n",
      "02/28 10:50:21 PM episode 25 score 43.0 avg score 27.2\n",
      "02/28 10:50:22 PM episode 26 score 27.0 avg score 27.2\n",
      "02/28 10:50:23 PM episode 27 score 40.0 avg score 27.6\n",
      "02/28 10:50:24 PM episode 28 score 22.0 avg score 27.4\n",
      "02/28 10:50:26 PM episode 29 score 35.0 avg score 27.7\n",
      "02/28 10:50:26 PM episode 30 score 21.0 avg score 27.5\n",
      "02/28 10:50:27 PM episode 31 score 14.0 avg score 27.1\n",
      "02/28 10:50:28 PM episode 32 score 28.0 avg score 27.1\n",
      "02/28 10:50:29 PM episode 33 score 20.0 avg score 26.9\n",
      "02/28 10:50:30 PM episode 34 score 40.0 avg score 27.3\n",
      "02/28 10:50:32 PM episode 35 score 66.0 avg score 28.3\n",
      "02/28 10:50:33 PM episode 36 score 17.0 avg score 28.0\n",
      "02/28 10:50:34 PM episode 37 score 21.0 avg score 27.8\n",
      "02/28 10:50:35 PM episode 38 score 29.0 avg score 27.9\n",
      "02/28 10:50:35 PM episode 39 score 16.0 avg score 27.6\n",
      "02/28 10:50:36 PM episode 40 score 13.0 avg score 27.2\n",
      "02/28 10:50:37 PM episode 41 score 15.0 avg score 26.9\n",
      "02/28 10:50:38 PM episode 42 score 47.0 avg score 27.4\n",
      "02/28 10:50:38 PM episode 43 score 17.0 avg score 27.2\n",
      "02/28 10:50:39 PM episode 44 score 33.0 avg score 27.3\n",
      "02/28 10:50:39 PM episode 45 score 22.0 avg score 27.2\n",
      "02/28 10:50:40 PM episode 46 score 23.0 avg score 27.1\n",
      "02/28 10:50:42 PM episode 47 score 58.0 avg score 27.7\n",
      "02/28 10:50:45 PM episode 48 score 78.0 avg score 28.8\n",
      "02/28 10:50:45 PM episode 49 score 21.0 avg score 28.6\n",
      "02/28 10:50:47 PM episode 50 score 39.0 avg score 28.8\n",
      "02/28 10:50:48 PM episode 51 score 32.0 avg score 28.9\n",
      "02/28 10:50:50 PM episode 52 score 48.0 avg score 29.2\n",
      "02/28 10:50:53 PM episode 53 score 75.0 avg score 30.1\n",
      "02/28 10:50:57 PM episode 54 score 108.0 avg score 31.5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 10.36 GiB already allocated; 1.50 MiB free; 10.36 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-87e7c1a986df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;31m#if not load_checkpoints:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mfinal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece271b_finalproject/inverted-pendulum-sac/sac_torch.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mq_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_states_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0mq1_old_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mq2_old_policy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mcritic1_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1_old_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-latest/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/ece271b_finalproject/inverted-pendulum-sac/networks.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0maction_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/ml-latest/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1061\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 10.36 GiB already allocated; 1.50 MiB free; 10.36 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import gym\n",
    "import numpy as np\n",
    "from sac_torch import Agent, Agent_sm\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join('tmp', 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "action_record = []\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('InvertedPendulumBulletEnv-v0')\n",
    "    #print(env.action_space.shape[0])\n",
    "    agent = Agent_sm(input_dims=env.observation_space.shape[0], env=env, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "    n_games = 200\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "    load_checkpoints = False\n",
    "\n",
    "    if load_checkpoints:\n",
    "        agent.load_models()\n",
    "        #env.render(mode='human')\n",
    "    \n",
    "    #save losses: final_loss, value_loss, actor_loss, critic_loss\n",
    "    final_loss = []\n",
    "    value_loss = []\n",
    "    actor_loss = []\n",
    "    critic_loss = []\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        step = 0\n",
    "        while not done:\n",
    "            action = agent.choose_action(observation)\n",
    "            action_record.append(action)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.remember(observation, action, reward, observation_, done)\n",
    "            #if not load_checkpoints:\n",
    "            l = agent.learn()\n",
    "            if l is not None:\n",
    "                final_loss.append(l[0])\n",
    "                value_loss.append(l[1])\n",
    "                actor_loss.append(l[2])\n",
    "                critic_loss.append(l[3])\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "#         if avg_score > best_score:\n",
    "#             best_score = avg_score\n",
    "#             #if not load_checkpoints:\n",
    "#             agent.save_models()\n",
    "        logging.info('episode %d score %.1f avg score %.1f', i, score, avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(value_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save losses: final_loss, value_loss, actor_loss, critic_loss\n",
    "n_games_sm = 50\n",
    "final_loss_sm = []\n",
    "value_loss_sm = []\n",
    "actor_loss_sm = []\n",
    "critic_loss_sm = []\n",
    "action_record_sm = []\n",
    "for i in range(n_games_sm):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    step = 0\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        action_record_sm.append(action)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        score += reward\n",
    "        agent.remember(observation, action, reward, observation_, done)\n",
    "        #if not load_checkpoints:\n",
    "        l = agent.learn_sm()\n",
    "        if l is not None:\n",
    "            final_loss_sm.append(l[0])\n",
    "            value_loss_sm.append(l[1])\n",
    "            actor_loss_sm.append(l[2])\n",
    "            critic_loss_sm.append(l[3])\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "#         if avg_score > best_score:\n",
    "#             best_score = avg_score\n",
    "#             #if not load_checkpoints:\n",
    "#             agent.save_models()\n",
    "    logging.info('episode %d score %.1f avg score %.1f', i, score, avg_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(actor_loss_sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(action_record_sm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-latest",
   "language": "python",
   "name": "ml-latest"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
